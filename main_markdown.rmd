---
title: "IML Term project"
author: "Julia Palorinne, Pyry Silomaa, Sara Sippola"
output: pdf_document
date: '`r format(Sys.Date(), "%d.%m.%Y")`'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(kableExtra)
```

```{r load data}
npfOrig <- read.csv("npf_train.csv")
```

```{r preprocess data}

# Dataset names:
# npfOrig          - Original data with has class2 as numeric values 0, 1
# npf          - "Original" data: has class2 with values 0,1 as numeric values, 
#                   columns removed: class4, partlybad, date (moved to rowname)
# npf.fact     - Otherwise the same as npf, but class2 is a factor with levels 0 and 1
# npf.168      - Processed data: Parameters that have different measurement heights have been reduced to height 16.8m
#                   columns removed: Parameters with height that's not 16.8m
# npf.168.fact -  Otherwise the same as npf.168, but class2 is a factor
# npfsp        - Reduced set of explanatory variables, received through running VIF analysis and repeating variable with highest VIF until no variable has a VIF
#                 higher than 10
# npfsp.fact   - Otherwise the same as npfsp but class2 is a factor


# Create class2, 0 = nonevent, 1 = event
npfOrig$class2 <- ifelse(npfOrig$class4 == "nonevent", 0, 1)

# Add date as row name
rownames(npfOrig) <- npfOrig[, "date"]

# Select global and 16.8 m high values
npf.168 <- data.frame(npfOrig[, (5:6)], npfOrig[, (13:16)], npfOrig[, (27:30)], 
                  npfOrig[, (41:42)], npfOrig[, (53:54)], npfOrig[, (63:72)],
                  npfOrig[, (83:88)], npfOrig[, (99:105)])

# Remove columns id, date, class4, partlybad from npf
npf <- npfOrig[, -(1:4)]

# Create datasets where class2 is a factor
npf.fact <- npf
npf.fact$class2 <- factor(npf.fact$class2)
npf.168.fact <- npf.168
npf.168.fact$class2 <- factor(npf.168.fact$class2)


# Divide numerical training data into two for training & testing
set.seed(42)
idx <- sample(nrow(npf), nrow(npf)/2)
npf_train <- npf[idx, ]
npf_test <- npf[-idx, ]

```

```{r functions}
## root mean squared error
rmse <- function(yhat, y) sqrt(mean((y - yhat)**2))

## Classification accuracy
cacc <- function(pred, true) sum(as.integer(as.logical(pred == true)))/length(pred)

## accuracy
accuracy <- function(pred, y) mean(ifelse(pred >= 0.5, 1, 0) == y)

## perplexity. Force probabilities to [0.995, 0.005]
perplexity <- function(pred, y){
  pred <- ifelse(pred != 0 & pred != 1, pred, ifelse(pred == 0, 0.005, 0.995 ))
  exp(-mean(log(ifelse(y == 1, pred, 1 - pred)))) }

# cross-validate
# split n items into k folds of roughly equal size
kpart <- function(n, k) {
    rep_len(1:k, length.out = n)
}

# Cross-validation predictor (from week 1 problem 2)¨
cv <- function(
    formula, # variables to use
    data, # data
    model = lm, # model to train
    n = nrow(data), # rows in the matrix
    k = min(n, 10), # cross-validation folds
    split = kpart(n, k), # the split of n data items into k folds
    train = function(data) model(formula, data = data),
    pred = function(model, data) predict(model, newdata = data)) {
  yhat <- NULL
  for (i in 1:k) {
    mod <- train(data[split != i, ])
    if (is.null(yhat)) {
      yhat <- pred(mod, data)
    } else {
      yhat[split == i] <- pred(mod, data[split == i, ])
    }
  }
  return(yhat)
}

loocv <- function(
    formula, # Formula specifying which variables to use
    data, # Dataset
    model = lm, # Type of model to train (as a function)
    ## function to train a model on data
    train = function(data) model(formula, data = data),
    ## function to make predictions on the trained model
    pred = function(model, data) predict(model, newdata = data)) {
  yhat <- NULL
  for (i in 1:nrow(data)) {
    ## go through all stations, train on other stations, and make a prediction
    mod <- train(data[-i, ])
    if (is.null(yhat)) {
      ## initialise yhat to something of correct data type,
      yhat <- pred(mod, data)
    } else {
      yhat[i] <- pred(mod, data[i, ])
    }
  }
  yhat # finally, output cross-validation predictions
}

# Find cross-validation predictions, return probabilities and not the classification
cv.prob <- function(
               formula, # Formula specifying which variables to use
               data, # Dataset
               model = lm, # Type of model to train (as a function)
               n = nrow(data), # number of rows in the data matrix
               k = min(n, 10), # number of cross-validation folds
               split = kpart(n, k), # the split of n data items into k folds
               ## function to train a model on data
               train = function(data) model(formula, data = data),
               ## function to make predictions on the trained model
               pred = function(model, data) predict(model, newdata = data, type = "raw")[,2]) {
    yhat <- NULL
    for (i in 1:k) {
        ## go through all folds, train on other folds, and make a prediction
        mod <- train(data[split != i, ])
        if (is.null(yhat)) {
            ## initialise yhat to something of correct data type,
            yhat <- pred(mod, data)
        } else {
            yhat[split == i] <- pred(mod, data[split == i, ])
        }
    }
    yhat # finally, output cross-validation predictions
}

loocv.prob <- function(
               formula, # Formula specifying which variables to use
               data, # Dataset
               model = lm, # Type of model to train (as a function)
               ## function to train a model on data
               train = function(data) model(formula, data = data),
               ## function to make predictions on the trained model
               pred = function(model, data) predict(model, newdata = data, type = "raw")[,2]) {
    yhat <- NULL
    for (i in 1:nrow(data)) {
        ## go through all stations, train on other stations, and make a prediction
        mod <- train(data[-i, ])
        if (is.null(yhat)) {
            ## initialise yhat to something of correct data type,
            yhat <- pred(mod, data)
        } else {
            yhat[i] <- pred(mod, data[i, ])
        }
    }
    yhat # finally, output cross-validation predictions
}
```

# Preprocessing data

## Correlation

Looking at the correlation matrix as it was presented in the solutions to Exercise set 1, it is clear that the measurements of the same thing at different heights correlate, as do some of the radiation-related parameters.

```{r correlation, message=FALSE}
library(corrplot)

# Calculate the correlation matrix
cm <- cor(npf[, endsWith(colnames(npf), ".mean")])

# Remove .mean from column and row names.
colnames(cm) <- rownames(cm) <- sapply(colnames(cm), function(s) gsub(".mean", "", s))

# Order variables by 1st principal component (PCA later in the course!)
corrplot(cm, order = "FPC", tl.cex = 0.5, tl.col = "black")
```

\newpage

### Correlation between measurements at different heights

Looking at the numeric values, we see that the measurements at different heights have strong positive correlation.

```{r correlation at different heights, message=FALSE}
# Correlation matrix
cm <- cor(npf[, endsWith(colnames(npf), ".mean")])
# Remove .mean from column and row names.
colnames(cm) <- rownames(cm) <- sapply(colnames(cm), function(s) gsub(".mean", "", s))
# Correlations for each variables (when more than one variable with the same beginning)
kable(cm[startsWith(colnames(cm), "CO2"), startsWith(colnames(cm), "CO2")],
      caption = "Correlation (CO2)") %>%
  kable_styling(latex_options = "HOLD_position")
kable(cm[startsWith(colnames(cm), "H2O"), startsWith(colnames(cm), "H2O")],
      caption = "Correlation (H20)") %>%
  kable_styling(latex_options = "HOLD_position")
kable(cm[startsWith(colnames(cm), "NO") & !startsWith(colnames(cm), "NOx"), startsWith(colnames(cm), "NO") & !startsWith(colnames(cm), "NOx")],
      caption = "Correlation (NO)") %>%
  kable_styling(latex_options = "HOLD_position")
kable(cm[startsWith(colnames(cm), "NOx"), startsWith(colnames(cm), "NOx")],
      caption = "Correlation (NOx)") %>%
  kable_styling(latex_options = "HOLD_position")
kable(cm[startsWith(colnames(cm), "O3"), startsWith(colnames(cm), "O3")],
      caption = "Correlation (O3)") %>%
  kable_styling(latex_options = "HOLD_position")
kable(cm[startsWith(colnames(cm), "RHIRGA"), startsWith(colnames(cm), "RHIRGA")],
      caption = "Correlation (RHIRGA)") %>%
  kable_styling(latex_options = "HOLD_position")
kable(cm[startsWith(colnames(cm), "T"), startsWith(colnames(cm), "T")],
      caption = "Correlation (T)") %>%
  kable_styling(latex_options = "HOLD_position")
```

\newpage

### Measurement heights

The measurement height 16.8m is the only one with all measurements. Because of this and the correlation between the measurements at different heights, we choose to discard measurements from heights other than 16.8m.

```{r measurementHeights, message=FALSE}
# Selecting which measurements have been done at particular heights
# Columns
cols <- unname(sapply(colnames(npf[endsWith(colnames(npf), ".mean")]), function(s) gsub(".mean", "", s)))
# Table of variables at different heights (only the variables that have been measured at different heights)
kable(data.frame(dm.42 = c(sort(cols[endsWith(cols, "42")])[1:6], NA, sort(cols[endsWith(cols, "42")])[7]),
                                dm.84 = c(NA, sort(cols[endsWith(cols, "84")])[1:5], NA, sort(cols[endsWith(cols, "84")])[6]),
                                dm.168 = c(sort(cols[endsWith(cols, "168")])),
                                dm.336 = c(sort(cols[endsWith(cols, "336")])[1:4], NA, sort(cols[endsWith(cols, "336")])[5], NA, NA),
                                dm.504 = c(sort(cols[endsWith(cols, "504")])[1:6], NA, sort(cols[endsWith(cols, "504")])[7]),
                                dm.672 = c(NA, NA, sort(cols[endsWith(cols, "672")])[2:5], NA, sort(cols[endsWith(cols, "672")])[6])),
      caption = "Measurements at different heights: What measurements have been done at particular heights?") %>%

  kable_styling(latex_options = "HOLD_position")
```

### vif
```{r vif}
library(car)

testmod<-glm(class2~., npf.168,family=binomial(link=logit))
inflfact<-vif(testmod)
max(inflfact)
which(inflfact==max(inflfact))

testmod<-glm(class2~.-PAR.mean-PAR.std, npf.168,family=binomial(link=logit))
inflfact<-vif(testmod)
max(inflfact)
which(inflfact==max(inflfact))

testmod<-glm(class2~.-PAR.mean-PAR.std-UV_A.std-UV_A.mean, npf.168,family=binomial(link=logit))
inflfact<-vif(testmod)
max(inflfact)
which(inflfact==max(inflfact))

testmod<-glm(class2~.-PAR.mean-PAR.std-UV_A.std-UV_A.mean-Glob.std-Glob.mean, npf.168,family=binomial(link=logit))
inflfact<-vif(testmod)
max(inflfact)
which(inflfact==max(inflfact))

testmod<-glm(class2~.-PAR.mean-PAR.std-UV_A.std-UV_A.mean-Glob.std-Glob.mean-UV_B.std-UV_B.mean, npf.168,family=binomial(link=logit))
inflfact<-vif(testmod)
max(inflfact)
which(inflfact==max(inflfact))

testmod<-glm(class2~.-PAR.mean-PAR.std-UV_A.std-UV_A.mean-Glob.std-Glob.mean-UV_B.std-UV_B.mean-RPAR.mean-RPAR.std, npf.168,family=binomial(link=logit))
inflfact<-vif(testmod)
max(inflfact)
which(inflfact==max(inflfact))

testmod <- glm(class2~.-PAR.mean-PAR.std-UV_A.std-UV_A.mean-Glob.std-Glob.mean-UV_B.std-UV_B.mean-RPAR.mean-RPAR.std-NET.mean-NET.std, npf.168,family=binomial(link=logit))
inflfact <- vif(testmod)
max(inflfact)
which(inflfact == max(inflfact))

testmod<-glm(class2~.-PAR.mean-PAR.std-UV_A.std-UV_A.mean-Glob.std-Glob.mean-UV_B.std-UV_B.mean-RPAR.mean-RPAR.std-NET.mean-NET.std-RGlob.std-RGlob.mean, npf.168,family=binomial(link=logit))
inflfact <- vif(testmod)
max(inflfact)
which(inflfact == max(inflfact))

testmod <- glm(class2~.-PAR.mean-PAR.std-UV_A.std-UV_A.mean-Glob.std-Glob.mean-UV_B.std-UV_B.mean-RPAR.mean-RPAR.std-NET.mean-NET.std-RGlob.std-RGlob.mean-CS.mean-CS.std, npf.168,family=binomial(link=logit))
inflfact <- vif(testmod)
max(inflfact)
which(inflfact == max(inflfact))

testmod <- glm(class2~.-PAR.mean-PAR.std-UV_A.std-UV_A.mean-Glob.std-Glob.mean-UV_B.std-UV_B.mean-RPAR.mean-RPAR.std-NET.mean-NET.std-RGlob.std-RGlob.mean-CS.mean-CS.std-NOx168.mean-NOx168.std, npf.168,family=binomial(link=logit))
inflfact <- vif(testmod)
max(inflfact)
which(inflfact==max(inflfact))
```

Runnig vif() over several variables in the 16.8m-restricted dataset, we come up to the following collection of coefficients 
```{r vif coefficients}
coefficients(testmod)
```

Hence we keep the following parameters
```{r vif parameters}
npfsp<-npf.168[,c(1:2,5:6,9:10,13:16,19:20,23:24,27:30,37)]
colnames(npfsp)
npfsp.fact<-npfsp
npfsp.fact[,19]<-as.factor(npfsp.fact[,19])
```

### PCA
#### PCA for all three versions of data 
```{r PCA}
pcaOrig <- prcomp(npf[,1:100], center = T, scale.=T)
cumsum(pcaOrig$sdev^2)/sum(pcaOrig$sdev^2)
x1 <- as.matrix(npf[,1:100])%*%pcaOrig$rotation[,1:12]
pca.168 <- prcomp(npf.168[,1:36], center = T, scale. = T)
cumsum(pca.168$sdev^2)/sum(pca.168$sdev^2)
x2 <- as.matrix(npf.168[,1:36])%*%pca.168$rotation[,1:12]
pca.sp <- prcomp(npfsp[,1:18], center = T, scale. = T)
cumsum(pca.sp$sdev^2)/sum(pca.sp$sdev^2)
x3 <- as.matrix(npfsp[,1:18])%*%pca.sp$rotation[,1:11]

npf.pca <- as.data.frame(cbind(x1, npf$class2))
colnames(npf.pca) <- c(colnames(x1), "class2")
npf.168.pca <- as.data.frame(cbind(x2, npf.168$class2))
colnames(npf.168.pca) <- c(colnames(x2), "class2")
```

# Classifiers

## Naive Bayes, LDA and QDA on the original data, 168-data and the PCA'd versions of these data sets

### Naive Bayes

```{r naive bayes}
library(e1071)

# Alkuperäinen data
nb.fit <- naiveBayes(class2 ~ . , data = npf_train)
nb.pred.cv <- cv.prob(class2 ~ ., npf, naiveBayes) # Hidas prosessi, tehdään siksi vaan kerran
nb.pred.loocv <- loocv.prob(class2 ~ ., npf, naiveBayes) # Sama

nb_acc <- data.frame(train = accuracy(predict(nb.fit, newdata = npf_train, type = "raw")[,2], npf_train$class2),
                     test = accuracy(predict(nb.fit, newdata = npf_test, type = "raw")[,2], npf_test$class2),
                     CV = accuracy(nb.pred.cv, npf$class2),
                     LOOCV = accuracy(nb.pred.loocv, npf$class2))

nb_perp <- data.frame(train = perplexity(predict(nb.fit, newdata = npf_train, type = "raw")[,2], npf_train$class2),
                      test = perplexity(predict(nb.fit, newdata = npf_test, type = "raw")[,2], npf_test$class2),
                      CV = perplexity(nb.pred.cv, npf$class2),
                      LOOCV = perplexity(nb.pred.loocv, npf$class2))
kable(cbind(nb_acc,
            nb_perp)) %>%
  add_header_above(c("Accuracy" = 4, "Perplexity" = 4)) %>%
  kable_styling(latex_options = "HOLD_position")

# 168-data
nb.pred.cv.168 <- cv.prob(class2 ~ ., npf.168, naiveBayes) # Hidas prosessi, tehdään siksi vaan kerran
nb.pred.loocv.168 <- loocv.prob(class2 ~ ., npf.168, naiveBayes) # Sama

nb_acc.168 <- data.frame(CV = accuracy(nb.pred.cv.168, npf.168$class2),
                     LOOCV = accuracy(nb.pred.loocv.168, npf.168$class2))

nb_perp.168 <- data.frame(CV = perplexity(nb.pred.cv.168, npf.168$class2),
                      LOOCV = perplexity(nb.pred.loocv.168, npf.168$class2))
kable(cbind(nb_acc.168,
            nb_perp.168)) %>%
  add_header_above(c("Accuracy" = 2, "Perplexity" = 2)) %>%
  kable_styling(latex_options = "HOLD_position")


```

Using PCA on the original and the 16.8m-dataset.

```{r naive bayes pca}

# Alkuperäinen data
nb.pred.cv.pca <- cv.prob(class2 ~ ., npf.pca, naiveBayes) # Hidas prosessi, tehdään siksi vaan kerran
nb.pred.loocv.pca <- loocv.prob(class2 ~ ., npf.pca, naiveBayes) # Sama

nb_acc.pca <- data.frame(CV = accuracy(nb.pred.cv.pca, npf.pca$class2),
                         LOOCV = accuracy(nb.pred.loocv.pca, npf.pca$class2))

nb_perp.pca <- data.frame(CV = perplexity(nb.pred.cv.pca, npf.pca$class2),
                          LOOCV = perplexity(nb.pred.loocv.pca, npf.pca$class2))
kable(cbind(nb_acc.pca,
            nb_perp.pca)) %>%
  add_header_above(c("Accuracy" = 2, "Perplexity" = 2)) %>%
  kable_styling(latex_options = "HOLD_position")

# 168-data
nb.pred.cv.168.pca <- cv.prob(class2 ~ ., npf.168.pca, naiveBayes) # Hidas prosessi, tehdään siksi vaan kerran
nb.pred.loocv.168.pca <- loocv.prob(class2 ~ ., npf.168.pca, naiveBayes) # Sama

nb_acc.168.pca <- data.frame(CV = accuracy(nb.pred.cv.168, npf.168.pca$class2),
                     LOOCV = accuracy(nb.pred.loocv.168, npf.168.pca$class2))

nb_perp.168.pca <- data.frame(CV = perplexity(nb.pred.cv.168, npf.168.pca$class2),
                      LOOCV = perplexity(nb.pred.loocv.168, npf.168.pca$class2))
kable(cbind(nb_acc.168.pca,
            nb_perp.168.pca)) %>%
  add_header_above(c("Accuracy" = 2, "Perplexity" = 2)) %>%
  kable_styling(latex_options = "HOLD_position")

```

### LDA

On the original dataset and the 16.8m-dataset.
```{r lda, warning=FALSE}
library(MASS)

# Cross-validation parametrit
n = nrow(npf) # number of rows in the data matrix
k = min(n, 10) # number of cross-validation folds
split = kpart(n, k) # the split of n data items into k folds

lda.pred.cv <- rep(0,n)
lda.pred.cv.168 <- rep(0,n)

# cv
for (i in 1:k) {
  ## go through all folds, train on other folds, and make a prediction
  mod <- lda(class2 ~ ., npf[split != i, ])
  lda.pred.cv[split == i] <- predict(mod, npf[split == i, ])$posterior[,2]
  ## 168
  mod <- lda(class2 ~ ., npf.168[split != i, ])
  lda.pred.cv.168[split == i] <- predict(mod, npf.168[split == i, ])$posterior[,2]
}

lda.pred.loocv <- rep(0,n)
lda.pred.loocv.168 <- rep(0,n)

# loocv
for(i in 1:dim(npf)[1]){
  mod <- lda(class2 ~ ., npf[split != i, ])
  lda.pred.loocv[split == i] <- predict(mod, npf[split == i, ])$posterior[,2]
  ## 168
  mod <- lda(class2 ~ ., npf.168[split != i, ])
  lda.pred.loocv.168[split == i] <- predict(mod, npf.168[split == i, ])$posterior[,2]
}

# npf
lda_acc <- data.frame(CV = accuracy(lda.pred.cv, npf$class2),
                      LOOCV = accuracy(lda.pred.loocv, npf$class2))
lda_perp <- data.frame(CV = perplexity(lda.pred.cv, npf$class2),
                       LOOCV = perplexity(lda.pred.loocv, npf$class2))
kable(cbind(lda_acc,
            lda_perp)) %>%
  add_header_above(c("Accuracy" = 2, "Perplexity" = 2))%>%
  kable_styling(latex_options = "HOLD_position")

# 168
lda_acc.168 <- data.frame(CV = accuracy(lda.pred.cv.168, npf$class2),
                          LOOCV = accuracy(lda.pred.loocv.168, npf$class2))
lda_perp.168 <- data.frame(CV = perplexity(lda.pred.cv.168, npf$class2),
                           LOOCV = perplexity(lda.pred.loocv.168, npf$class2))
kable(cbind(lda_acc.168,
            lda_perp.168)) %>%
  add_header_above(c("Accuracy" = 2, "Perplexity" = 2))%>%
  kable_styling(latex_options = "HOLD_position")
```

Using PCA on the original and the 16.8m-dataset.

```{r lda pca, warning=FALSE}

# Cross-validation parametrit
n = nrow(npf) # number of rows in the data matrix
k = min(n, 10) # number of cross-validation folds
split = kpart(n, k) # the split of n data items into k folds

lda.pred.cv.pca <- rep(0,n)
lda.pred.cv.168.pca <- rep(0,n)

# cv
for (i in 1:k) {
  ## go through all folds, train on other folds, and make a prediction
  mod <- lda(class2 ~ ., npf.pca[split != i, ])
  lda.pred.cv.pca[split == i] <- predict(mod, npf.pca[split == i, ])$posterior[,2]
  ## 168
  mod <- lda(class2 ~ ., npf.168.pca[split != i, ])
  lda.pred.cv.168.pca[split == i] <- predict(mod, npf.168.pca[split == i, ])$posterior[,2]
}

lda.pred.loocv.pca <- rep(0,n)
lda.pred.loocv.168.pca <- rep(0,n)

# loocv
for(i in 1:dim(npf)[1]){
  mod <- lda(class2 ~ ., npf.pca[split != i, ])
  lda.pred.loocv.pca[split == i] <- predict(mod, npf.pca[split == i, ])$posterior[,2]
  ## 168
  mod <- lda(class2 ~ ., npf.168.pca[split != i, ])
  lda.pred.loocv.168.pca[split == i] <- predict(mod, npf.168.pca[split == i, ])$posterior[,2]
}

# npf
lda_acc.pca <- data.frame(CV = accuracy(lda.pred.cv.pca, npf.pca$class2),
                          LOOCV = accuracy(lda.pred.loocv.pca, npf.pca$class2))
lda_perp.pca <- data.frame(CV = perplexity(lda.pred.cv.pca, npf.pca$class2),
                           LOOCV = perplexity(lda.pred.loocv.pca, npf.pca$class2))
kable(cbind(lda_acc.pca,
            lda_perp.pca)) %>%
  add_header_above(c("Accuracy" = 2, "Perplexity" = 2))%>%
  kable_styling(latex_options = "HOLD_position")

# 168
lda_acc.168.pca <- data.frame(CV = accuracy(lda.pred.cv.168.pca, npf.168.pca$class2),
                              LOOCV = accuracy(lda.pred.loocv.168.pca, npf.168.pca$class2))
lda_perp.168.pca <- data.frame(CV = perplexity(lda.pred.cv.168.pca, npf.168.pca$class2),
                               LOOCV = perplexity(lda.pred.loocv.168.pca, npf.168.pca$class2))
kable(cbind(lda_acc.168.pca,
            lda_perp.168.pca)) %>%
  add_header_above(c("Accuracy" = 2, "Perplexity" = 2))%>%
  kable_styling(latex_options = "HOLD_position")
```

### QDA
```{r qda, warning=FALSE}
library(MASS)

# Cross-validation parametrit
n = nrow(npf) # number of rows in the data matrix
k = min(n, 10) # number of cross-validation folds
split = kpart(n, k) # the split of n data items into k folds

qda.pred.cv <- rep(0,n)
qda.pred.cv.168 <- rep(0,n)

# cv
for (i in 1:k) {
  ## go through all folds, train on other folds, and make a prediction
  mod <- qda(class2 ~ ., npf[split != i, ])
  qda.pred.cv[split == i] <- predict(mod, npf[split == i, ])$posterior[,2]
  ## 168
  mod <- qda(class2 ~ ., npf.168[split != i, ])
  qda.pred.cv.168[split == i] <- predict(mod, npf.168[split == i, ])$posterior[,2]
}

qda.pred.loocv <- rep(0,n)
qda.pred.loocv.168 <- rep(0,n)

# loocv
for(i in 1:dim(npf)[1]){
  mod <- qda(class2 ~ ., npf[split != i, ])
  qda.pred.loocv[split == i] <- predict(mod, npf[split == i, ])$posterior[,2]
  ## 168
  mod <- qda(class2 ~ ., npf.168[split != i, ])
  qda.pred.loocv.168[split == i] <- predict(mod, npf.168[split == i, ])$posterior[,2]
}

# npf
qda_acc <- data.frame(CV = accuracy(qda.pred.cv, npf$class2),
                      LOOCV = accuracy(qda.pred.loocv, npf$class2))
qda_perp <- data.frame(CV = perplexity(qda.pred.cv, npf$class2),
                       LOOCV = perplexity(qda.pred.loocv, npf$class2))
kable(cbind(qda_acc,
            qda_perp)) %>%
  add_header_above(c("Accuracy" = 2, "Perplexity" = 2))%>%
  kable_styling(latex_options = "HOLD_position")

# 168
qda_acc.168 <- data.frame(CV = accuracy(qda.pred.cv.168, npf$class2),
                          LOOCV = accuracy(qda.pred.loocv.168, npf$class2))
qda_perp.168 <- data.frame(CV = perplexity(qda.pred.cv.168, npf$class2),
                           LOOCV = perplexity(qda.pred.loocv.168, npf$class2))
kable(cbind(qda_acc.168,
            qda_perp.168)) %>%
  add_header_above(c("Accuracy" = 2, "Perplexity" = 2))%>%
  kable_styling(latex_options = "HOLD_position")
```

Using PCA on the original and the 16.8m-dataset.

```{r qda pca, warning=FALSE}

# Cross-validation parametrit
n = nrow(npf) # number of rows in the data matrix
k = min(n, 10) # number of cross-validation folds
split = kpart(n, k) # the split of n data items into k folds

qda.pred.cv.pca <- rep(0,n)
qda.pred.cv.168.pca <- rep(0,n)

# cv
for (i in 1:k) {
  ## go through all folds, train on other folds, and make a prediction
  mod <- qda(class2 ~ ., npf.pca[split != i, ])
  qda.pred.cv.pca[split == i] <- predict(mod, npf.pca[split == i, ])$posterior[,2]
  ## 168
  mod <- qda(class2 ~ ., npf.168.pca[split != i, ])
  qda.pred.cv.168.pca[split == i] <- predict(mod, npf.168.pca[split == i, ])$posterior[,2]
}

qda.pred.loocv.pca <- rep(0,n)
qda.pred.loocv.168.pca <- rep(0,n)

# loocv
for(i in 1:dim(npf)[1]){
  mod <- qda(class2 ~ ., npf.pca[split != i, ])
  qda.pred.loocv.pca[split == i] <- predict(mod, npf.pca[split == i, ])$posterior[,2]
  ## 168
  mod <- qda(class2 ~ ., npf.168.pca[split != i, ])
  qda.pred.loocv.168.pca[split == i] <- predict(mod, npf.168.pca[split == i, ])$posterior[,2]
}

# npf
qda_acc.pca <- data.frame(CV = accuracy(qda.pred.cv.pca, npf.pca$class2),
                          LOOCV = accuracy(qda.pred.loocv.pca, npf.pca$class2))
qda_perp.pca <- data.frame(CV = perplexity(qda.pred.cv.pca, npf.pca$class2),
                           LOOCV = perplexity(qda.pred.loocv.pca, npf.pca$class2))
kable(cbind(qda_acc.pca,
            qda_perp.pca)) %>%
  add_header_above(c("Accuracy" = 2, "Perplexity" = 2))%>%
  kable_styling(latex_options = "HOLD_position")

# 168
qda_acc.168.pca <- data.frame(CV = accuracy(qda.pred.cv.168.pca, npf.168.pca$class2),
                              LOOCV = accuracy(qda.pred.loocv.168.pca, npf.168.pca$class2))
qda_perp.168.pca <- data.frame(CV = perplexity(qda.pred.cv.168.pca, npf.168.pca$class2),
                               LOOCV = perplexity(qda.pred.loocv.168.pca, npf.168.pca$class2))
kable(cbind(qda_acc.168.pca,
            qda_perp.168.pca)) %>%
  add_header_above(c("Accuracy" = 2, "Perplexity" = 2))%>%
  kable_styling(latex_options = "HOLD_position")
```

### GLM
```{r glm, warning=FALSE, fig.height=4}
mod1<-glm(class2~., npf, family=binomial(link="logit"))
mod2<-glm(class2~., npf, family=binomial(link="probit"))
plot(mod1)
prob1<-predict(mod1, npf, type="response")
pred1<-c()
perp1<-c()
for(i in 1:dim(npf)[1]){
  if(prob1[i]>=0.5){
    pred1<-c(pred1,1)
  } else{
    pred1<-c(pred1,0)
  }
  if(npf$class2[i]==1){
    perp1<-c(perp1,prob1[i])
  } else{
    perp1<-c(perp1,1-prob1[i])
  }
}
```

GLM accuracy and perplexity. 
```{r glm res, warning=FALSE}
mean(pred1==npf$class2)
exp(-mean(perp1))
```

### Lasso / Ridge
```{r lasso, message=FALSE, fig.height=4}
library(glmnet)
mod3<-cv.glmnet(x=as.matrix(npf[,1:100]), y=npf[,101],alpha=1, family="binomial", type.measure="class")
mod4<-cv.glmnet(x=as.matrix(npf[,1:100]), y=npf[,101],alpha=0, family="binomial", type.measure = "class")
mod3a<-glmnet(x=as.matrix(npf[,1:100]), y=npf[,101],alpha=1, family="binomial", lambda=mod3$lambda.min)
coef(mod3)
plot(mod3a)
plot(mod3$glmnet.fit, xvar="lambda")
prob3a<-predict(mod3a, newx=as.matrix(npf[,1:100]), type="response")
pred3a<-c()
perp3a<-c()
for(i in 1:dim(npf)[1]){
  if(prob3a[i]>=0.5){
    pred3a<-c(pred3a,1)
  } else{
    pred3a<-c(pred3a,0)
  }
  if(npf$class2[i]==1){
    perp3a<-c(perp3a,prob3a[i])
  } else{
    perp3a<-c(perp3a,1-prob3a[i])
  }
}
```

Lasso accuracy and perplexity. 
```{r lasso  res, warning=FALSE}
mean(pred3a==npf$class2)
exp(-mean(perp3a))
```

```{r ridge, message=FALSE, fig.height=4}
mod4a<-glmnet(x=as.matrix(npf[,1:100]), y=npf[,101],alpha=0, family="binomial", lambda=mod4$lambda.min)
plot(mod4a)
plot(mod4$glmnet.fit, xvar="lambda")
prob4a<-predict(mod4a, newx=as.matrix(npf[,1:100]), type="response")
pred4a<-c()
perp4a<-c()
for(i in 1:dim(npf)[1]){
  if(prob4a[i]>=0.5){
    pred4a<-c(pred4a,1)
  } else{
    pred4a<-c(pred4a,0)
  }
  if(npf$class2[i]==1){
    perp4a<-c(perp4a,prob4a[i])
  } else{
    perp4a<-c(perp4a,1-prob4a[i])
  }
}
```

Ridge accuracy and perplexity. 
```{r ridge res, warning=FALSE}
mean(pred4a==npf$class2)
exp(-mean(perp4a))
```

### SVM
```{r svm, warning=FALSE}
modsv<-tune(svm, class2~., data=npf.fact, kernel="linear", ranges=list(cost=c(0.001, 0.01, 0.1, 1, 2, 5)))
modsvm<-svm(class2~., data=npf.fact, kernel="linear", scale=FALSE, cost=0.001,probability=TRUE)
predsv<-predict(modsvm,newdata=npf.fact)
predsv2<-predict(modsvm,newdata=npf.fact,probability = T)
probsv<-attr(predsv2, "probabilities")
perpsv<-c()
for(i in 1:dim(npf)[1]){
  if(npf$class2[i]==0){
    perpsv<-c(perpsv, probsv[i,2])
  } else{
    perpsv<-c(perpsv, probsv[i,1])
  }
}
```
SVM accuracy and perplexity. 
```{r SVM res, warning=FALSE}
mean(predsv==npf$class2)
exp(-mean(log(perpsv)))
```

### Tree-based methods

```{r trees, message=FALSE, warning=FALSE}
library(tree)
library(MASS)
library(randomForest) # random forest
library(gbm) # GBM
library (ISLR2)

#'Dummy model (from week 1 problem 2)
dummy <- function(formula, data) {
  target <- all.vars(formula[[2]])
  lm(as.formula(sprintf("%s ~ 1", target)), data)
}

# Models
models <- list(
  Dummy = dummy,
  RandomForest = randomForest,
  GBM = gbm
)

# gmb_pred <- predict(gbm(class2 ~ ., data = npf, cv.folds = 10, train.fraction = 0.5), data=npf)
# print(cacc(gmb_pred, npf$class2))

# Errors
errors <- sapply(models, function(model) {
  mod <- model(class2 ~ ., data = npf_train)
  c(
    c(
      train = rmse(predict(mod, data = npf_train), npf_train$class2),
      test = rmse(predict(mod, data = npf_test), npf_test$class2),
      CV = rmse(cv(mod, data = npf_train), npf_train$class2),
      LOOCV = rmse(loocv(mod, data = npf_train), npf_train$class2) 
    )
  )
})

# Classification accuracies
results <- sapply(models, function(model) {
  mod <- model(class2 ~ ., data = npf_train)
  c(
    c(
      train = cacc(predict(mod, data = npf_train), npf_train$class2),
      test = cacc(predict(mod, data = npf_test), npf_test$class2),
      CV = cacc(cv(mod, data = npf_train), npf_train$class2),
      LOOCV = cacc(loocv(mod, data = npf_train), npf_train$class2)
    )
  )
})

kable(errors,
      caption = "Tree-based methods, errors") %>%
  kable_styling(latex_options = "HOLD_position")

kable(results,
      caption = "Tree-based methods, classification accuracy") %>%
  kable_styling(latex_options = "HOLD_position")
```

### Lasso (first version of final model)
```{r lasso2}
lasso1 <- cv.glmnet(x = as.matrix(npf[,1:100]), y = npf$class2, alpha=1, family="binomial", type.measure="class", nfolds=10)
lassomod1 <- glmnet(x=as.matrix(npf[,1:100]),y = npf$class2, alpha = 1, lambda = lasso1$lambda.min, family="binomial", type.measure="class")
preds<-c()
perps<-c()
for(i in 1:464){
  newexp <- npf[-i,]
  newmod <- glmnet(x=as.matrix(newexp[,1:100]), y=newexp[,101], alpha=1,family="binomial", lambda = lasso1$lambda.min)
  newprob <- predict(newmod, newx=as.matrix(npf[i,1:100]),type="response")
  if (npf$class2[i] == 1) {
    newperp = log(newprob)
  } else {
    newperp = log(1-newprob)
  }
  perps <- c(perps,newperp)
  if (newprob > 0.5) {
    newpred <- 1
  } else{
    newpred <- 0
  }
  preds <- c(preds, newpred == npfsp$class2[i])
}
mean(preds == npf$class2)
exp(-mean(perps))
```

### GLM (first version of actual model without usage of PCAs):
```{r glm2}
logreg <- glm(class2~., npfsp, family=binomial())
logreg <- glm(class2~.-O3168.mean-O3168.std, npfsp, family=binomial())
logreg <- glm(class2~.-O3168.mean-O3168.std-PTG.mean-PTG.std, npfsp, family=binomial())
logreg <- glm(class2~.-O3168.mean-O3168.std-PTG.mean-PTG.std-Pamb0.mean-H2O168.std-SO2168.mean-SO2168.std-1, npfsp, family=binomial())
preds<-c()
perps<-c()
for (i in 1:464) {
  newexp <- npfsp[-i,]
  newmod <- glm(class2~.-O3168.mean-O3168.std-PTG.mean-PTG.std-Pamb0.mean-H2O168.std-SO2168.mean-SO2168.std-1, data=npfsp, family=binomial())
  newprob <- predict(newmod, newdata=npfsp[i,], type="response")
  if (npf$class2[i] == 1) {
    newperp = log(newprob)
  } else {
    newperp = log(1-newprob)
  }
  perps <- c(perps,newperp)
  if (newprob > 0.5) {
    newpred <- 1
  } else {
    newpred <- 0
  }
  preds <- c(preds, newpred == npfsp$class2[i])
}
mean(preds)
exp(-mean(perps))
```

### SVM (first version of actual model)
```{r svm2, eval=FALSE}
tc <- tune.control(cross=464)
tuner <- tune(svm.class2~., data=npf.168.fact, kernel="linear", type="C", ranges=list(cost=seq(0.01,0.05,0.01)), tunecontrol=tc)
bpam <- tuner$best.parameters
preds <- c()
perps<-c()
for(i in 1:464){
  newexp <- npf.168.fact[-i,]
  newmod <- svm(class2~., newexp,kernel="linear", cost=bpam[1,1], scale=F, probability=T, type="C")
  newprob <- predict(newmod, newdata=npf.168.fact[i,1:36], probability=T)
  if (class2[i] == 1) {
    newperp <- attr(newprob, "probabilities")[1,1]
  } else {
    newperp <- attr(newprob, "probabilities")[1,2]
  }
  perps <- c(perps,log(newperp))
  preds <- c(preds, newprob[1])
}
mean((preds-1) == class2)
exp(-mean(perps))
```

